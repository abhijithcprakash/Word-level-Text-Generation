{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install pydota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pickle import dump, load\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import add\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"cleaned_captions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(dataset_path, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a child in'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in each lines extracted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = text.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Already cleaned and now is a list of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vocabulary(captions_list):\n",
    "    \n",
    "    # Initialize an empty set to store the vocabulary\n",
    "    vocab = set()\n",
    "\n",
    "    # Iterate over each caption in the list\n",
    "    for caption in captions_list:\n",
    "        # Split the caption into words and add them to the vocabulary set\n",
    "        vocab.update(caption.split())\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = text_vocabulary(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8828"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding the <#start> and <#end> tokens to the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end_tokens(sentences):\n",
    "\n",
    "    sentences_with_tokens = []\n",
    "\n",
    "    # Iterate over each sentence in the list\n",
    "    for sentence in sentences:\n",
    "        # Add start token '<start>' to the beginning of the sentence and end token '<end>' to the end\n",
    "        sentence_with_tokens = '<start> ' + sentence + ' <end>'\n",
    "        # Append the modified sentence to the list\n",
    "        sentences_with_tokens.append(sentence_with_tokens)\n",
    "\n",
    "    return sentences_with_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the add_start_end_tokens function with the sentences list\n",
    "sentences_with_tokens = add_start_end_tokens(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_with_tokens = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(sentences_with_tokens):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences_with_tokens)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8829"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(sentences_with_tokens)\n",
    "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate maximum length of descriptions\n",
    "\n",
    "def max_length(sentences_with_tokens):\n",
    "    \n",
    "    return max(len(d.split()) for d in sentences_with_tokens)\n",
    "\n",
    "max_length = max_length(sentences_with_tokens)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x2d5bbf406d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting list of sentencees into dictionary with key as numbers and values as group 5 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sentences_into_lists(sentences, group_size):\n",
    "    \n",
    "    grouped_lists = []\n",
    "    for i in range(0, len(sentences), group_size):\n",
    "        grouped_lists.append(sentences[i:i+group_size])\n",
    "    return grouped_lists\n",
    "\n",
    "def convert_to_dictionary(sentences, group_size):\n",
    "    \n",
    "    grouped_sentences = group_sentences_into_lists(sentences, group_size)\n",
    "    dictionary = {}\n",
    "    for i, group in enumerate(grouped_sentences, start=1):\n",
    "        dictionary[i] = group\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 5  # Define the group size\n",
    "\n",
    "# Convert sentences to a dictionary\n",
    "sentences_dictionary = convert_to_dictionary(sentences_with_tokens, group_size)\n",
    "\n",
    "# Print the dictionary\n",
    "for key, value in sentences_dictionary.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for key, description_list in descriptions.items():\n",
    "\n",
    "            input_sequence, output_word = create_sequences(tokenizer, max_length, description_list)\n",
    "            yield [[input_sequence], output_word]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, desc_list):\n",
    "    X1, y = list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            \n",
    "            X1.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((58, 38), (58, 8829))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a],b = next(data_generator(sentences_dictionary, tokenizer, max_length))\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,    3],\n",
       "       [   0,    0,    0, ...,    0,    3,    1],\n",
       "       [   0,    0,    0, ...,    3,    1,   42],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  314,   64,    1],\n",
       "       [   0,    0,    0, ...,   64,    1,  194],\n",
       "       [   0,    0,    0, ...,    1,  194, 2992]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### MOdel 1 --- think need to train for more epoch tested only after 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "\n",
    "# # define the captioning model\n",
    "# def define_model(vocab_size, max_length):\n",
    "\n",
    "#     # LSTM sequence model\n",
    "#     inputs1 = Input(shape=(max_length,))\n",
    "#     se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs1)\n",
    "#     se2 = Dropout(0.5)(se1)\n",
    "#     se3 = LSTM(256)(se2)\n",
    "\n",
    "#     decoder2 = Dense(256, activation='relu')(se3)\n",
    "#     outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "#     # tie it together [image, seq] [word]\n",
    "#     model = Model(inputs=[inputs1], outputs=outputs)\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "#     # summarize model\n",
    "#     print(model.summary())\n",
    "#     plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Modle 2 chatgpt improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_model(vocab_size, max_length):\n",
    "\n",
    "#     # LSTM sequence model\n",
    "#     inputs1 = Input(shape=(max_length,))\n",
    "#     se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs1)\n",
    "#     se2 = Dropout(0.5)(se1)\n",
    "#     se3 = LSTM(1024, return_sequences=True)(se2)  # Adding return_sequences=True to return sequences for the next LSTM layer\n",
    "#     se4 = Dropout(0.5)(se3)  # Adding dropout after the LSTM layer\n",
    "#     se5 = LSTM(1024)(se4)  # Adding another LSTM layer\n",
    "#     decoder1 = Dense(512, activation='relu')(se5)  # Adding a dense layer after the LSTM\n",
    "#     decoder2 = Dense(512, activation='relu')(decoder1)\n",
    "#     decoder3 = Dense(512, activation='relu')(decoder2)\n",
    "#     outputs = Dense(vocab_size, activation='softmax')(decoder3)\n",
    "    \n",
    "#     # tie it together [image, seq] [word]\n",
    "#     model = Model(inputs=[inputs1], outputs=outputs)\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "#     # summarize model\n",
    "#     print(model.summary())\n",
    "#     # plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "\n",
    "    # LSTM sequence model\n",
    "    inputs1 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs1)\n",
    "    se2 = Dropout(0.3)(se1)\n",
    "    se3 = LSTM(1024, return_sequences=True)(se2)\n",
    "    se4 = Dropout(0.3)(se3)\n",
    "    se5 = LSTM(1024)(se4)\n",
    "    decoder1 = Dense(512, activation='relu')(se5)\n",
    "    dropout1 = Dropout(0.3)(decoder1)  # Adding dropout after the first dense layer\n",
    "    decoder2 = Dense(512, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.3)(decoder2)  # Adding dropout after the second dense layer\n",
    "    decoder3 = Dense(512, activation='relu')(dropout2)\n",
    "    dropout3 = Dropout(0.3)(decoder3)  # Adding dropout after the third dense layer\n",
    "    outputs = Dense(vocab_size, activation='softmax')(dropout3)\n",
    "    \n",
    "    model = Model(inputs=[inputs1], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 38)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 38, 256)           2260224   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 38, 256)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 38, 1024)          5246976   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 38, 1024)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1024)              8392704   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8829)              4529277   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,479,293\n",
      "Trainable params: 21,479,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions: train= 8092\n",
      "Vocabulary Size: 8829\n",
      "Description Length:  38\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 38)]              0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 38, 256)           2260224   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 38, 256)           0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 38, 1024)          5246976   \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 38, 1024)          0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 1024)              8392704   \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 8829)              4529277   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,479,293\n",
      "Trainable params: 21,479,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\AppData\\Local\\Temp\\ipykernel_17644\\826986174.py:13: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8092/8092 [==============================] - 3530s 435ms/step - loss: 4.3405\n",
      "8092/8092 [==============================] - 3630s 449ms/step - loss: 3.7765\n",
      "8092/8092 [==============================] - 4826s 596ms/step - loss: 3.6129\n",
      "8092/8092 [==============================] - 3578s 442ms/step - loss: 3.5283\n",
      "8092/8092 [==============================] - 3701s 457ms/step - loss: 3.4748\n",
      "8092/8092 [==============================] - 3437s 425ms/step - loss: 3.4426\n",
      "8092/8092 [==============================] - 3437s 425ms/step - loss: 3.4162\n",
      "8092/8092 [==============================] - 3439s 425ms/step - loss: 3.4011\n",
      "8092/8092 [==============================] - 3442s 425ms/step - loss: 3.3870\n",
      "8092/8092 [==============================] - 3450s 426ms/step - loss: 3.3765\n"
     ]
    }
   ],
   "source": [
    "# train our model\n",
    "print('Descriptions: train=', len(sentences_dictionary))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 10\n",
    "steps = len(sentences_dictionary)\n",
    "# making a directory models to save our models\n",
    "# os.mkdir(\"models\")\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(sentences_dictionary, tokenizer, max_length)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n",
    "    model.save(\"models/textmodel_\" + str(i) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"WordLevelTextGenModel10.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = \"a girl\"\n",
    "predict_next_words= 30\n",
    "# # model = load_model(\"WordLevelTextGenModel.h5\")\n",
    "\n",
    "# for _ in range(predict_next_words):\n",
    "#     token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
    "#     print(token_list)\n",
    "#     token_list = pad_sequences([token_list], maxlen=max_length, padding='pre')\n",
    "#     predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "#     output_word = \"\"\n",
    "#     for word, index in tokenizer.word_index.items():\n",
    "#         if index == predicted:\n",
    "#             output_word = word\n",
    "#             break\n",
    "#     input_text += \" \" + output_word\n",
    "\n",
    "# print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_text = \"a man\"\n",
    "\n",
    "for _ in range(predict_next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    print(token_list)\n",
    "    token_list = pad_sequences([token_list], maxlen=max_length, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            if word == \"end\":  # Check if the predicted word is the end token\n",
    "                break  # Stop text generation if the end token is predicted\n",
    "            output_word = word\n",
    "            break\n",
    "    input_text += \" \" + output_word\n",
    "\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhij\\miniconda3\\envs\\tensorflow2.10\\lib\\site-packages\\datasets\\load.py:753: FutureWarning: The repository for bleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/bleu/bleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.8091067115702212,\n",
       " 'precisions': [0.8571428571428571, 0.8333333333333334, 0.8, 0.75],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.0,\n",
       " 'translation_length': 7,\n",
       " 'reference_length': 7}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "bleu = load_metric(\"bleu\")\n",
    "predictions = [[\"two\", \"dogs\", \"are\", \"running\", \"through\",\"the\", \"water\"]]\n",
    "references = [\n",
    "    [[\"Two\", \"dogs\", \"are\", \"running\", \"through\",\"the\", \"water\"]]\n",
    "]\n",
    "\n",
    "bleu.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1 : 0.8571428571428571\n",
      "BLEU-2 : 0.8451542547285166\n",
      "BLEU-3 : 0.8298265333662435\n",
      "BLEU-4 : 0.8091067115702212\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "precisions = [0.8571428571428571, 0.8333333333333334, 0.8, 0.75]\n",
    "brevity_penalty = 1.0\n",
    "\n",
    "# BLEU-1\n",
    "bleu_1 = precisions[0]\n",
    "\n",
    "# BLEU-2\n",
    "bleu_2 = (precisions[0] * precisions[1]) ** 0.5\n",
    "\n",
    "# BLEU-3\n",
    "bleu_3 = (precisions[0] * precisions[1] * precisions[2]) ** (1/3)\n",
    "\n",
    "# BLEU-4\n",
    "bleu_4 = (precisions[0] * precisions[1] * precisions[2] * precisions[3]) ** 0.25\n",
    "\n",
    "print(\"BLEU-1 :\", bleu_1)\n",
    "print(\"BLEU-2 :\", bleu_2)\n",
    "print(\"BLEU-3 :\", bleu_3)\n",
    "print(\"BLEU-4 :\", bleu_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.9 (tensorflow2.10)",
   "language": "python",
   "name": "tensorflow2.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
